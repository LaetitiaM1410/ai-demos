{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure\n",
    "!pip install base64\n",
    "!pip install openai\n",
    "!pip install re\n",
    "!pip install fitz\n",
    "!pip install typing\n",
    "!pip install PyMuPDF\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79639699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd567eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "azure_doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "client = DocumentAnalysisClient(azure_doc_intelligence_endpoint, AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "input_filepath = \"\"#insert path of page_1.pdf here\n",
    "with open(input_filepath, \"rb\") as f:\n",
    "    poller = client.begin_analyze_document(\"prebuilt-read\", document=f)\n",
    "    result = poller.result()\n",
    "\n",
    "full_text = \"\"\n",
    "for page in result.pages:\n",
    "    for line in page.lines:\n",
    "        full_text += line.content + \"\\n\"\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508be173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz\n",
    "\n",
    "input_filepath = \"\"#insert path of page_1.pdf here\n",
    "doc = fitz.open(input_filepath)\n",
    "page = doc[0]\n",
    "pix = page.get_pixmap(dpi=150)\n",
    "pix.save(\"page1.png\")\n",
    "\n",
    "with open(\"page1.png\", \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"\"\"You are an intelligent document analyst.\n",
    "Given this document page, identify all images, figures or tables.\n",
    "For each figure:\n",
    "1. Provide a detailed description of what the figure or image shows.\n",
    "2. For the location just provide what the next few lines of text say, be precise (at least 2 sentences)!. If you cant find sentences afterwards (if the figure is at the end of the page) provide the previous sentences.\n",
    "\n",
    "Just return the structured list for each image/figure/table based on what you found ALWAYS use this structure:\n",
    "[Description:...\n",
    "Location:...]\n",
    ".\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"You are a document editor.\n",
    "\n",
    "Given a full document and a block of sentences that appear shortly after a figure, your job is to find where in the document these sentences appear or match most closely, and insert the following figure description **just before** them.\n",
    "Respond with the new version of the document that has the description inserted in the correct place.\n",
    "Insert the figure description and add **Figure description** before it.\n",
    "---\n",
    "\n",
    "Figure Description and Location:\n",
    "{result}\n",
    "\n",
    "---\n",
    "\n",
    "Document:\n",
    "{full_text}\n",
    ".\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bd489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
